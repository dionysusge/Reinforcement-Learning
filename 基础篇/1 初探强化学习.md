# 基础篇

## 初探

### 简介

强化学习属于机器学习领域的**“序贯决策**”任务

和预测任务不同，决策往往会有“后果”，因而决策者要根据后果的情况优化判断

该篇主要讨论强化学习的基本概念和思维方式，了解要解决什么任务，如何用数学进行刻画，学习的目标是什么

### 什么是强化学习

“**强化学习”**：一个迭代过程，根据当前状态做决策 -> 决策动作用于环境中 -> 环境发生改变，给予智能体相应奖励 -> 根据当前状态做决策…..

**“智能体”**：感知环境信息，通过决策来直接改变这个环境

那么智能体的关键要素就为：**感知、决策、奖励**

强化学习的**环境状态**，由两部分组成，一个是环境自身的演变，另一个是外来的干扰因素，即智能体的动作

### 强化学习的目标

奖励即为给智能体决策的反馈信号，整个交互过程，每一轮的奖励进行叠加，就是**整体回报**

但即使初始环境、智能体策略不变，交互产生的结果可能也是不同的，因此关注**整体回报的期望**，称为**价值**，这便是优化的目标

### 强化学习中的数据

有监督的学习，是在给定的数据集上训练，使得损失函数最小

强化学习，数据是在和环境的交互中得到的，如果智能体不作为，那么该动作对应的数据就永远得不到

**占用度量**：归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。

强化学习的思维方式

- 策略在训练中会不断更新，其对应的数据分布（即占用度量）也会不断改变。因而强化学习的一个难点在于，数据分布是随智能体的学习而不断改变的
- 奖励建立在状态动作对上，寻找最优策略即代表着寻找最优占用度量

### 强化学习的独特性

有监督的学习

目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的泛化误差（generalization error）

![image-20250417105337594](https://raw.githubusercontent.com/dionysusge/MyPic/refs/heads/img/img/image-20250417105337594.png)

强化学习任务的最终优化目标是最大化智能体策略在和动态环境交互过程中的价值。

![image-20250417105343867](https://raw.githubusercontent.com/dionysusge/MyPic/refs/heads/img/img/image-20250417105343867.png)

观察以上两个优化公式，总结出两者的相似点和不同点。

- 有监督学习和强化学习的优化目标相似，即都是在优化某个数据分布下的一个分数值的期望。
- 二者优化的途径是不同的，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即**修改目标函数而数据分布不变**；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即**修改数据分布而目标函数不变。**这里的目标函数即为状态-行为奖励对。