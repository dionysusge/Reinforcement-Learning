# Markov decision process， MDP

# 一、简介
    # 强化学习中的重要概念，与多臂老虎机不同，马尔可夫决策过程包含了状态信息和状态之间的转移机制

    # 如果要用强化学习去解决一个实际问题，那么第一步要做的事情，就是把这个实际问题抽象为一个马尔可夫决策过程
    # 抽象为马尔可夫决策过程，那么就要明确过程的各个组成要素。

    # 下面先从马尔可夫过程触发，最后引出马尔可夫决策过程

# 二、马尔可夫过程
    # 1、随机过程
        # 随机过程为“概率学”的“动力学”部分。概率论的研究对象是静态的随机现象
        # 随机过程的研究对象是随时间变化的随机对象
        # 随机过程中，随机现象在某个时刻t的取值为向量随机变量，用S_t表示
        # 所有可能的状态组成状态集合S。
        # 随机现象便是状态的变化过程，在某时刻t的状态通常取决于t时刻之前的状态

        # 下一个状态S_(t+1)的概率可以抽象为以下数学形式
        # P(S_(t+1)|S_1, S_2...S_t)

    # 2、马尔可夫性质
        # 当某时刻的状态只取决于上一时刻的状态时，该随机过程就具有“马尔可夫性质”
        # 学术上表达为：当前状态是未来的充分统计量，即下一个状态只取决于当前状态。
        # 但是并不是说和过去的状态没有关系，只不过它是“链式”的影响，上一个状态其实也有要上上个状态决定，历史的信息链式传递了
        # 这种情况下，运算会大大地简化，利用当前状态就可以决定下一个状态，而非过去所有状态的累计


    # 3、马尔可夫过程
        # 指具有马尔可夫性质的随机过程，也叫马尔可夫链
        # 可以用一个元组（S，P）去描述，S是有限数量的状态集合，P是状态转移矩阵。
        # 假设有n个状态，那么状态转移矩阵的大小就是n * n，刻画了从不同状态转移到该状态的概率大小，包括概率为0
        # 第i行第j列的值就代表从状态i转移到状态j的概率

        # 在马尔可夫过程中，存在：不会转移到其它状态的状态，称为终止状态。可以理解为它永远以概率1转移到自己

# 三、马尔可夫奖励过程
    # 在马尔可夫过程的基础上，加入奖励函数r和折扣因子γ，就得到了马尔可夫奖励过程
    # 也即，马尔可夫奖励过程由（S,P,r,γ）组成

    # 奖励函数r指的是：转移到该状态时，可以获得奖励的期望
    # 折扣因子范围在[0, 1)，这是为了平衡长短期利益，当我们期望更快地获得一些奖励时，折扣因子接近0，而接近1的折扣因子更关注长期奖励

    # 1、回报
        # 在马尔可夫奖励过程中，回报指的是，从t时刻状态S_t开始，到终止状态时，所有奖励的衰减之和，用G_t表示
        # G_t = R_t + γ * R_(t+1) + γ^2 * R_(t+2) + .... + γ^(k) * R_(t+k)
        # 奖励函数为0时，时序列终止，计算回报

        # 下面编码实现马尔可夫奖励过程，并实现计算回报的函数

import numpy as np
np.random.seed(0)
# 定义状态转移概率矩阵P
P = [
    [0.9, 0.1, 0.0, 0.0, 0.0, 0.0],
    [0.5, 0.0, 0.5, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.6, 0.0, 0.4],
    [0.0, 0.0, 0.0, 0.0, 0.3, 0.7],
    [0.0, 0.2, 0.3, 0.5, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 1.0],
]
P = np.array(P)

rewards = [-1, -2, -2, 10, 1, 0]  # 定义奖励函数
gamma = 0.5  # 定义折扣因子


# 给定一条序列,计算从某个索引（起始状态）开始到序列最后（终止状态）得到的回报
def compute_return(start_index, chain, gamma):
    G = 0
    for i in reversed(range(start_index, len(chain))):
        G = gamma * G + rewards[chain[i] - 1]
    return G


# 一个状态序列,s1-s2-s3-s6
chain = [1, 2, 3, 6]
start_index = 0
G = compute_return(start_index, chain, gamma)
print("根据本序列计算得到回报为：%s。" % G)

    # 2、价值函数
        # 在马尔可夫过程中，一个状态的期望回报称为这个状态的“价值”
        # 该状态的期望回报指的是，从这个状态出发，未来累计奖励的期望，由于终止状态的存在，未来累计奖励的期望是可以计算的
        # 所有状态的价值就构成了价值函数，输入为某个状态，输出为该状态的价值

        # 那么某个状态的价值，可以通过该状态出发的奖励，加上下一个状态的价值去描述
        # 最终得到的形式，是一阶方程（称为贝尔曼方程）
        # V = (I - γP)^(-1)R
        # 可以通过矩阵运算，得到价值函数的 解析解

        # 计算复杂度是o(n^3)，n是状态的个数
        # 上述方法适用于较小的马尔可夫奖励过程，如果规模较大，可以用以下算法求解价值函数，在之后的章节会介绍
            # 蒙特卡洛方法
            # 动态规划算法
            # 时序差分算法

        # 下面编码实现求解价值函数的解析解方法

def compute(P, rewards, gamma, states_num):
    ''' 利用贝尔曼方程的矩阵形式计算解析解,states_num是MRP的状态数 '''
    rewards = np.array(rewards).reshape((-1, 1))  #将rewards写成列向量形式
    value = np.dot(np.linalg.inv(np.eye(states_num, states_num) - gamma * P),
                   rewards)
    return value


V = compute(P, rewards, gamma, 6)
print("MRP中每个状态价值分别为\n", V)

